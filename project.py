# -*- coding: utf-8 -*-
"""Another copy of project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xJtkKiI-PAz3oMyRbtZ7_hAbCKlndwqN

# ⚡💨 **Wind Turbine Power Prediction**

## <font color="royalblue">Overview</font>  
Predicting the **power output** of a wind turbine using **machine learning**.  
Dataset = 8760 hourly records with:  
-  <font color="green">Power Generated (kW)</font> → *Target*  
-  Wind Speed (m/s)  
-  Wind Direction (°)  
-  Temperature (°C)  
-  Pressure (atm)  

##  <font color="darkorange">Goal</font>  
- Build ML models to estimate **power output**  
- Identify key environmental factors  

##  <font color="purple">Methods</font>  
·  EDA  
·  Evaluation with RMSE, MAE, R²  

##  <font color="red">Outcome</font>  
- Accurate prediction of turbine power  
- Insights into factors affecting generation
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # Evaluation metrics
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor   # Ensemble models
from sklearn.neighbors import KNeighborsRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import time
from sklearn.linear_model import Ridge
from xgboost import XGBRegressor

"""##  <font color="royalblue">Dataset Columns</font>

 <font color="orange">Time stamp:</font> Date & time of measurement (hourly data, full year)  

 <font color="green">System power generated (kW):</font> Target variable → energy produced by the wind turbine  

 <font color="blue">Wind speed (m/s):</font> Main predictor, directly affects turbine power  

 <font color="teal">Wind direction (deg):</font> Angle of wind flow, may influence turbine efficiency  

 <font color="darkred">Air temperature (°C):</font> Weather factor affecting air density & power  

 <font color="purple">Pressure (atm):</font> Atmospheric pressure, can impact wind characteristics  

"""

df = pd.read_csv("/content/TexasTurbine.csv")
df

"""##**Exploratory Data Analysis (EDA)**

## 📊 <font color="royalblue">Data Info</font>

 <font color="green">All columns:</font> 8760 non-null values (no missing data)  
 <font color="orange">'Time stamp':</font> Object type → needs conversion to datetime  
 <font color="blue">Other columns:</font> Numeric (float / int)
"""

df.info()

"""## 📈 <font color="royalblue">Descriptive Stats</font>

 <font color="green">Power generated:</font> ~500 → ~2000 kW  
 <font color="blue">Wind speed:</font> Mostly 5 → 15 m/s  
 <font color="darkorange">Temperature:</font> Around 18–20 °C  
 <font color="purple">Outliers:</font> No extreme outliers visible  

"""

df.describe()

"""## 🔍 <font color="royalblue">Data Quality Check</font>

 <font color="orange">Missing Values:</font>  
  * No missing values were found in any column.  


 <font color="green">Duplicate Rows:</font>  
 *  No duplicate rows exist in the dataset.  

"""

# Check for missing values in each column
print(df.isnull().sum() ,'\n')
##____________________________________________
 ##Check for duplicate rows
print("Duplicates:", df.duplicated().sum())

"""## <font color="royalblue"> Outliers detected</font>  

-  <font color="green">Numeric Features:</font> **`System power`, `Wind speed`, `Wind direction`, `Pressure`,` Temperature`**  
-  <font color="red">Outliers:</font> Few detected, but not far from the box  
-  Likely natural variations in wind or weather conditions  
-  <font color="purple">Decision:</font> Keep the outliers as they represent real-world fluctuations

"""

# Plot boxplots for all numeric features to check distribution and outliers

df[['System power generated | (kW)'	,'Wind speed | (m/s)',	'Wind direction | (deg)',	'Pressure | (atm)',	"Air temperature | ('C)"]].plot(kind='box', figsize=(12,6))
plt.show()

"""### 🕒 <font color="royalblue">Time Feature Engineering</font>  

- Added **Year (2023)** to timestamps  
- Converted text to <font color="purple">datetime object</font>  
- Extracted features:  
  - Hour  
  - Day  
  - Month  
- Helps ML model learn **seasonal + daily patterns**

"""

# Add the year (2023)
df["Time stamp"] = "2023 " + df["Time stamp"].astype(str)

# Choose the format of the date
df["Time stamp"] = pd.to_datetime(df["Time stamp"], format="%Y %b %d, %I:%M %p")

# Extract the features from the date
df["hour"] = df["Time stamp"].dt.hour
df["month"] = df["Time stamp"].dt.month
df["day"] = df["Time stamp"].dt.day
df

"""### <font color="royalblue">Correlation Matrix</font>  

- Shows pairwise correlation between numerical features  
- Strong positive correlation expected between  **`Wind Speed ↔ Power Generated`**
- Watch out for <font color="red">multicollinearity</font> between independent variables  
- Useful for **feature selection + model insights**


"""

#The correlation Matrix
corr = df.corr()

plt.figure(figsize=(8,6))
sns.heatmap(corr, annot=True, cmap="Purples", fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

"""##  <font color="royalblue">Features & Target</font>

- 🔹 <font color="green">Features (X):</font>  
  Variables used by the model to make predictions → Wind Speed, Month, Hour, Wind Direction, Temperature, Pressure  
- 🔹 <font color="orange">Target (y):</font>  
  The variable we want the model to predict → System Power Generated (kW)  

##  <font color="purple">Train/Test Split</font>

- Split the data into **Train/Test** to evaluate model performance on unseen data  
- Test size: 25% → 25% of the data used for testing  
- Random state: 42 → ensures the **same split every time**  


"""

features = ["Wind speed | (m/s)","month","hour", "Wind direction | (deg)", "Pressure | (atm)"]
X = df[features]
y = df["System power generated | (kW)"]

# spliting the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

print("Train shape:", X_train.shape, y_train.shape)
print("Test shape:", X_test.shape, y_test.shape)

"""##  <font color="darkorange">Feature Scaling</font>

- We use <font color="green">MinMaxScaler</font> to scale both **features (X)** and **target (y)**  
- Transforms the data to the range **[0, 1]**  
- Helps the neural networks and gradient-based models converge faster and perform more stably  

"""

# Scaling
scaler_X = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

"""##  <font color="darkred">Ridge Regression</font>

- Using <font color="green">Ridge Regression</font> to predict system power  
- Alpha = 1.0 → regularization strength  
- Fit the model on **scaled training data**  
- Predict on **scaled testing data**

###  Evaluation
- R²: <font color="blue">0.9105</font> → The model explains ~91% of the variance in the test data  
- RMSE: <font color="purple">0.0854</font> → Average prediction error on the test set  
- Training Time: <font color="red">0.0135 sec</font> → Very fast, low computational complexity  

"""

# Ridge Regression

from sklearn.linear_model import Ridge

start = time.time()
ridge = Ridge(alpha=1.0, random_state=42)
ridge.fit(X_train_scaled, y_train)


y_pred_ridge = ridge.predict(X_test_scaled)
end = time.time()
ridge_time = end - start
ridge_r2 = r2_score(y_test, y_pred_ridge)
ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))

print(f"Ridge R2: {ridge_r2}")
print(f"Ridge RMSE: {ridge_rmse}")
print(f"Ridge training time: {ridge_time} sec")

"""##  <font color="darkgreen">XGBoost Regressor</font>

- Using <font color="green">XGBRegressor</font> to predict system power  
- Parameters:  
  - n_estimators = 300 → number of trees  
  - learning_rate = 0.05 → step size shrinkage  
  - max_depth = 6 → maximum tree depth  
  - random_state = 42 → reproducibility  
- Fit the model on **training data**  
- Predict on **testing data**

###  Evaluation
- R²: <font color="blue">0.99996</font> → Model explains **~99.99%** of the variance in the test data  
- RMSE: <font color="purple">0.0017 (normalized units)</font> → Very low average prediction error  
- Training Time: <font color="red">0.391 sec</font> → Relatively fast given the model’s complexity  

"""

# XGBoost
from xgboost import XGBRegressor
start = time.time()
xgb = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=6, random_state=42)

xgb.fit(X_train_scaled, y_train)

y_pred_xgb = xgb.predict(X_test_scaled)
end = time.time()
xgb_time = end - start
xgb_r2 = r2_score(y_test, y_pred_xgb)
xgb_rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
print(f"XGBoost R2: {xgb_r2}")
print(f"XGBoost RMSE: {xgb_rmse}")
print(f"XGBoost training time: {xgb_time} sec")

"""##  <font color="darkred">Neural Network (MLPRegressor)</font>

- Using <font color="green">MLPRegressor</font> to predict system power  
- Architecture: **2 hidden layers** → (64, 32) neurons  
- Activation: <font color="blue">ReLU</font>  
- Solver: <font color="purple">Adam</font>  
- max_iter = 500, random_state = 42  

###  Evaluation
- R²: <font color="blue">0.9948</font> → Model explains **~99.5%** of the variance in the test data  
- RMSE: <font color="purple">0.0206 (normalized units)</font> → Low average prediction error  
- Training Time: <font color="red">0.567 sec</font> → Slower than Ridge/XGBoost but still efficient  

"""

# Neural Network
from sklearn.neural_network import MLPRegressor
start = time.time()
mlp = MLPRegressor(hidden_layer_sizes=(64,32), activation='relu', solver='adam',
                   max_iter=500, random_state=42)

mlp.fit(X_train_scaled, y_train.ravel())  # flatten y

y_pred_mlp = mlp.predict(X_test_scaled)
end = time.time()
mlp_time = end - start
mlp_r2 = r2_score(y_test, y_pred_mlp)
mlp_rmse = np.sqrt(mean_squared_error(y_test, y_pred_mlp))


print(f"Neural Net R2: {mlp_r2}")
print(f"Neural Net RMSE: {mlp_rmse}")
print(f"Neural Net training time: {mlp_time} sec")

"""
- Using <font color="green">Keras Sequential Model</font> with LSTM to predict system power  
- Architecture: **1 LSTM layer** → (50 units) + **1 Dense layer** (output)  
- Optimizer: <font color="blue">Adam</font>  
- Loss Function: <font color="purple">MSE</font>  

###  Evaluation  
- R²: <font color="blue">0.9986</font> → Model explains **~99.9%** of the variance in the test data  
- RMSE: <font color="purple">0.0108 (normalized units)</font> → Very low average prediction error  
- Training Time: <font color="red">29.64 sec</font> → Significantly slower due to sequential LSTM computations  
"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping


# ---------------------
# 1. Scaling X & y
# ---------------------


scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1,1))
y_test_scaled = scaler_y.transform(y_test.values.reshape(-1,1))

# reshape X for LSTM [samples, timesteps, features]
X_train_lstm = np.expand_dims(X_train_scaled, axis=-1)
X_test_lstm = np.expand_dims(X_test_scaled, axis=-1)

# ---------------------
# 2. Build stronger LSTM model
# ---------------------
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(X_train_lstm.shape[1],1)),
    Dropout(0.2),
    LSTM(32),
    Dense(1)
])
model.compile(optimizer="adam", loss="mse")

# ---------------------
# 3. Train model with EarlyStopping
# ---------------------
es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

start = time.time()
history = model.fit(
    X_train_lstm, y_train_scaled,
    validation_split=0.1,
    epochs=200,
    batch_size=32,
    callbacks=[es],
    verbose=0
)


# ---------------------
# 4. Predict & inverse transform
# ---------------------
y_pred_scaled = model.predict(X_test_lstm)
y_pred_lstm = scaler_y.inverse_transform(y_pred_scaled)
end = time.time()
# ---------------------
# 5. Evaluate
# ---------------------
lstm_r2 = r2_score(y_test, y_pred_lstm)
lstm_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lstm))
lstm_time = end - start

print(f"LSTM R2: {lstm_r2:.4f}")
print(f"LSTM RMSE: {lstm_rmse:.4f}")
print(f"LSTM training time: {lstm_time:.4f} sec")

# List of models and their predictions
models = ["Ridge", "XGBoost", "Neural Net", "LSTM"]
predictions = [y_pred_ridge, y_pred_xgb, y_pred_mlp, y_pred_lstm]
times = [ridge_time, xgb_time, mlp_time, lstm_time]

# Print header
print(f"{'Model':<20} {'R²':<18} {'RMSE':<20} {'Time (s)':<16}")
print("-"*75)

# Loop through each model
for model_name, y_pred, t in zip(models, predictions, times):
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    print(f"{model_name:<12} {r2:<20} {rmse:<20} {t:<20}")

"""**<font color="darkred">Model Performance Visualization</font>**

The figure below compares the four models:
<font color="blue">Ridge</font>, <font color="green">XGBoost</font>, <font color="orange">Neural Net</font>, and <font color="purple">LSTM</font>.
Three evaluation aspects are shown side-by-side:

 <font color="blue">R² Score per Model</font>

Measures how well each model explains the variance in the test data.

Closer to 1 is better.

All models achieved very high R² (>0.99), with LSTM slightly outperforming the others.

 <font color="green">RMSE per Model</font>

Shows the average prediction error in normalized units.

Lower values are better.

LSTM achieved the lowest RMSE, followed by XGBoost, while Ridge and Neural Net were slightly higher.

 <font color="orange">Training Time per Model</font>

Indicates efficiency in model training.

Ridge trained almost instantly.

XGBoost and Neural Net required more time but stayed under 1 second.

LSTM was much slower (~30 seconds) due to its recurrent architecture.

 <font color="purple">Takeaway</font>

LSTM → Best accuracy, but slowest training.

Ridge → Fastest, but slightly less accurate.

XGBoost → Excellent trade-off between speed and accuracy.

Neural Net (MLP) → High accuracy, but slower than Ridge/XGBoost.
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import r2_score, mean_squared_error

# ---------------------
# 1. Compare results of all models
# ---------------------
models = ["Ridge", "XGBoost", "Neural Net", "LSTM"]

r2_scores = [
    r2_score(y_test, y_pred_ridge),
    r2_score(y_test, y_pred_xgb),
    r2_score(y_test, y_pred_mlp),
    r2_score(y_test, y_pred_lstm)  # LSTM after inverse_transform
]

rmses = [
    np.sqrt(mean_squared_error(y_test, y_pred_ridge)),
    np.sqrt(mean_squared_error(y_test, y_pred_xgb)),
    np.sqrt(mean_squared_error(y_test, y_pred_mlp)),
    np.sqrt(mean_squared_error(y_test, y_pred_lstm))
]

times = [
    ridge_time,
    xgb_time,
    mlp_time,
    lstm_time  # LSTM training time measured correctly
]

# ---------------------
# 2. Colors
# ---------------------
cmap = plt.get_cmap("viridis")
colors = [cmap(i / len(models)) for i in range(len(models))]

# ---------------------
# 3. Plot
# ---------------------
plt.figure(figsize=(18,5))

# R² Score
plt.subplot(1,3,1)
bars = plt.bar(models, r2_scores, color=colors)
plt.ylim(min(0, min(r2_scores)-0.05), 1.05)
plt.title("R² Score per Model", fontsize=14, fontweight="bold")
plt.ylabel("R²", fontsize=12)
for bar, score in zip(bars, r2_scores):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.01,
             f"{score:.3f}", ha='center', va='bottom', fontsize=10)

# RMSE
plt.subplot(1,3,2)
bars = plt.bar(models, rmses, color=colors)
plt.title("RMSE per Model", fontsize=14, fontweight="bold")
plt.ylabel("RMSE (original units)", fontsize=12)
for bar, rmse in zip(bars, rmses):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.01,
             f"{rmse:.3f}", ha='center', va='bottom', fontsize=10)

#  Time
plt.subplot(1,3,3)
bars = plt.bar(models, times, color=colors)
plt.title("Time per Model", fontsize=14, fontweight="bold")
plt.ylabel("Time (sec)", fontsize=12)
for bar, t in zip(bars, times):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.01,
             f"{t:.3f}", ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()

"""##  Actual vs Predicted Comparison  

The scatter plots below show **actual power (x-axis)** vs. **predicted power (y-axis)** for each model.  
The closer the points are to the <font color="red">red dashed line</font> (perfect prediction line), the better the model.  

- <font color="purple">Ridge Regression</font> → Predictions are close but show a bit more spread.  
- <font color="gold">XGBoost</font> → Strong fit with tighter clustering near the diagonal.  
- <font color="orange">Neural Network (MLP)</font> → Very accurate, points align well.  
- <font color="green">LSTM</font> → Best fit, predictions nearly overlap with the diagonal line.  

"""

# Actual values
y_true = y_test   # no need for .values if y_test is already Series/array

# Models info
models = ["Ridge Regression", "XGBoost", "Neural Network", "LSTM"]
preds = [y_pred_ridge, y_pred_xgb, y_pred_mlp, y_pred_lstm]
colors = ["blue", "green", "orange", "purple"]

# Plot
plt.figure(figsize=(20,5))

for i, (model, pred, color) in enumerate(zip(models, preds, colors), 1):
    plt.subplot(1,4,i)
    plt.scatter(y_true, pred, alpha=0.5, color=color)
    plt.plot([y_true.min(), y_true.max()],
             [y_true.min(), y_true.max()],
             'r--', linewidth=2)
    plt.xlabel("Actual Power (kW)", fontsize=10)
    plt.ylabel("Predicted Power (kW)", fontsize=10)
    plt.title(model, fontsize=12, fontweight="bold")

plt.tight_layout()
plt.show()